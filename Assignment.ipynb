{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QX-8p5b7_vv"
      },
      "source": [
        "## Hello!\n",
        "\n",
        "About the Take Home Assignment:\n",
        "The aim of the Take Home Assisnment is to check your understanding of AI-ML concepts, and consists of two simple questions. \n",
        "For each question you are expected to do the following:\n",
        "1) Preprocess the data.\n",
        "2) Analyse the data (Explain your understanding / observations)\n",
        "3) Create a model and justify the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wYyS_XQCjJv"
      },
      "source": [
        "# Logic\n",
        "**Q1**\n",
        "---\n",
        "I have worked with classifiers like naive bayes earlier as well. I knew that by encoding each unique word by an integer(here I chose it's frequency since now I could distinguish between sentences differing in only word frequency like \"good\" and \"good good\" ) I could classify each summary record with a unique feature set and a label(Score)\n",
        "\n",
        "Now to improve predictions I put score above 3 to be positive else negative\n",
        "So I had 2 classes and numerical input features. After this I used the Multinomial Naive Bayes(multinomial since input features are non binary) to classify the record\n",
        "Before running on the model I cleaned/processed the data( removed the Nan values and reduced the dimensions by feature selection)\n",
        "\n",
        "---\n",
        "**Q2**\n",
        "---\n",
        "I first observed the data and discovered the different classes of flowers and counted them to be 5 and encoded them in integer values\n",
        "Later I build a pandas data frame with the images (decoded using tensor flow libraries) and their corresponding class(flower encoding)\n",
        "I randomly took records  from this dataframe and split them in training testing set\n",
        "Then I build a keras NN model. I used CNN to capture various subtle features of each image( color, shapes etc) \n",
        "The first layer was rescaling each color value to range of 0-1 to save computation\n",
        "Second layer was a convolutional followed by activation layer with relu function ( this improved non linearity of the model). \n",
        "I used 3 convo layers followed by a max pooling to decrease the parameters. Then I flattened all the feature maps after convolution, passed them through a hidden layer and condensed then to final 5 classes.\n",
        "\n",
        "I first got accuracy of 30 %, this was when my training data was small ( about 800) and only 1 convo layer. I gradually increased dataset and added some layers to its final output \n",
        "\n",
        "I avoided data augmentation since training and testing the given data was only taking more time.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Olv6_a7KhW1"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "\n",
        "Download the Amazon Fine Food Reviews Dataset (https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?select=Reviews.csv) and build a model to predict the sentiment of the review.\n",
        "\n",
        "You must start with basic EDA and pre-processing (Explain each step of the model building process)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNHXNz21CZLz"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BRAJyH3KnQa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/Reviews.csv',engine=\"python\",on_bad_lines='skip')\n",
        "# we drop the duplicate values since they are already analysed before\n",
        "df.drop_duplicates(subset = \"Summary\",inplace=True)\n",
        "df.drop_duplicates(subset = \"Id\",inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6yaClB6cHI9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E68pbRX3QpAo",
        "outputId": "7df08039-9aba-45a3-a725-072b68c859c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 295743 entries, 0 to 568452\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count   Dtype \n",
            "---  ------                  --------------   ----- \n",
            " 0   Id                      295743 non-null  int64 \n",
            " 1   ProductId               295743 non-null  object\n",
            " 2   UserId                  295743 non-null  object\n",
            " 3   ProfileName             295737 non-null  object\n",
            " 4   HelpfulnessNumerator    295743 non-null  int64 \n",
            " 5   HelpfulnessDenominator  295743 non-null  int64 \n",
            " 6   Score                   295743 non-null  int64 \n",
            " 7   Time                    295743 non-null  int64 \n",
            " 8   Summary                 295742 non-null  object\n",
            " 9   Text                    295743 non-null  object\n",
            "dtypes: int64(5), object(5)\n",
            "memory usage: 24.8+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArcFfuyJQq0E",
        "outputId": "aebb91fc-0274-4d67-f993-ec805d6f0f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 295742 entries, 0 to 568452\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count   Dtype \n",
            "---  ------   --------------   ----- \n",
            " 0   UserId   295742 non-null  object\n",
            " 1   Score    295742 non-null  int64 \n",
            " 2   Summary  295742 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 9.0+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# We dont need all columns(features) for sentinent analysis so we do dimensionality reduction by feature selection\n",
        "# we drop the rows that containe na values and get the CLEANED Data\n",
        "# We do sentinent analysis on the Summary of the review(it is already clean and has visual link with the score)\n",
        "df.drop(df.columns[[0,1,3,4,5,7,9]],axis=1,inplace=True)\n",
        "df.dropna(subset = \"Summary\",inplace=True)\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b51lhHxuXmKU",
        "outputId": "338e95de-aa16-4c87-cf73-c1b384411dd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the text to analyse has been cleaned\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pprint\n",
        "\n",
        "# Function to clean the text by removing the words/characters not used in sentiment analysis\n",
        "def clean_text(text_array):\n",
        "    cleaned_array = []\n",
        "    for text in text_array:\n",
        "        # Remove non-ASCII characters\n",
        "        cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "        # Remove non-alphanumeric characters\n",
        "        cleaned_text = re.sub(r'[^a-zA-Z0-9]+', ' ', cleaned_text).replace('.','')\n",
        "        # Convert to lowercase\n",
        "        cleaned_text = cleaned_text.lower()\n",
        "        cleaned_array.append(cleaned_text)\n",
        "    return cleaned_array\n",
        "\n",
        "df['Summary'] = clean_text(df['Summary'])\n",
        "print(\"the text to analyse has been cleaned\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-cYYrfVKrzn"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o-eMBIfG2KL",
        "outputId": "344f9e65-e70c-4113-f2f7-ad7f699bb9bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here the feature set of a record will contain the frequency of each word in the text\n"
          ]
        }
      ],
      "source": [
        "# Well use bag of words Vectorization to build a Numerical Labelled Feature set after which we use Multinomial Naive Bayes to make sentinent prediction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bag = vectorizer.fit_transform(df['Summary'])\n",
        "\n",
        "bags = bag.toarray()\n",
        "print(\"here the feature set of a record will contain the frequency of each word in the text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udlL1o3JLGeA"
      },
      "outputs": [],
      "source": [
        "# We take random 10000 records, train the sampled dataset and predict on the model\n",
        "index = np.random.choice(bags.shape[0], size=10000, replace=False)\n",
        "# arr = [i for i in index]\n",
        "# print(arr)\n",
        "arr = [bags[i] for i in index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9g__5kgI_qd"
      },
      "outputs": [],
      "source": [
        "# we build 2 classes: above 3 rating as positive sentiment and below 4 as negative sentinent\n",
        "df.loc[df[\"Score\"] <= 3, \"Score\"] = 0\n",
        "df.loc[df[\"Score\"] > 3 , \"Score\"] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kda60Uo_QLLn",
        "outputId": "730ebe0d-3b70-417e-d349-6c8c3d7f5d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.758\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into 80% train and 20% test data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(arr, df['Score'][:10000], test_size=0.80, random_state=90)\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# We use a multinomial Naive Bayes Classifier\n",
        "clf = MultinomialNB(alpha=1)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train,Y_train)\n",
        "\n",
        "# clf.predict(X_test)\n",
        "\n",
        "# Gives the accuracy\n",
        "print(clf.score(X_test, Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hDHF2f9KsGY"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "\n",
        "Download the Flower Classification Dataset (\"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\") and build a model to predict the flower.\n",
        "You should also show the following: 1) Display 4 samples of each type of flower 2) Convert images to grayscale and test the model. 3) Resize the images.\n",
        "\n",
        "You must start with basic EDA and pre-processing (Explain each step of the model building process)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNCBqz-IOze6"
      },
      "outputs": [],
      "source": [
        "## Helper Code to get you started:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,losses\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1ze1v_oO7Q8",
        "outputId": "7e3a7232-2bfd-4bff-b9d7-9701f5ff428f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PosixPath('/root/.keras/datasets/flower_photos/tulips'), PosixPath('/root/.keras/datasets/flower_photos/LICENSE.txt'), PosixPath('/root/.keras/datasets/flower_photos/sunflowers'), PosixPath('/root/.keras/datasets/flower_photos/daisy'), PosixPath('/root/.keras/datasets/flower_photos/roses'), PosixPath('/root/.keras/datasets/flower_photos/dandelion')]\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "print(list(data_dir.glob('*')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtmZL32Uv8bS",
        "outputId": "b9f61dc5-0de2-4dca-d9ec-c4cbc780c7df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thus there are 5 classes of flowers => ['roses','dandelion','daisy','sunflowers','tulips']\n",
            "{'daisy': 0, 'dandelion': 1, 'roses': 2, 'sunflowers': 3, 'tulips': 4}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "print(\"Thus there are 5 classes of flowers => ['roses','dandelion','daisy','sunflowers','tulips']\")\n",
        "classes = ['roses','tulips','dandelion','daisy','sunflowers']\n",
        "le = LabelEncoder()\n",
        "le.fit(classes)\n",
        "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "# We encode the different labels for a nn can only work with encoded classes\n",
        "print(le_name_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_Ri7IFzPAdN",
        "outputId": "f82929c5-3bfb-45d4-ac4b-1062d0839c80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3670\n"
          ]
        }
      ],
      "source": [
        "All_images = list(data_dir.glob('*/*.jpg'))\n",
        "print(len(All_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0qk3GTCPFG6"
      },
      "outputs": [],
      "source": [
        "# prepare the dataset, it contains the img matrix( tensored ) and is labeled \n",
        "import pandas as pd\n",
        "\n",
        "def readImage(imagePath):\n",
        "  # read the image and re scale it to range 0-1 to avoid computation cost\n",
        "  img = tf.io.read_file(str(imagePath))\n",
        "  img = tf.image.decode_jpeg(img)\n",
        "  img = tf.image.resize(img, size = (256,256))\n",
        "  # img = np.array(plt.imread(str(imagePath))) / 255.0\n",
        "  return img\n",
        "\n",
        "# The data frame is randomized and a subset of it is taken as the whole set is computationally extensive\n",
        "temp = {'image':[],'Class':[]}\n",
        "# for img in np.random.choice(len(All_images),size=360, replace=False):\n",
        "#   temp['image'].append(readImage(All_images[img]))\n",
        "#   temp['Class'].append(le_name_mapping[str(All_images[img]).split('/')[-2]])\n",
        "np.random.shuffle(All_images)\n",
        "for img in All_images:\n",
        "  temp['image'].append(readImage(img))\n",
        "  temp['Class'].append(le_name_mapping[str(img).split('/')[-2]])\n",
        "\n",
        "df = pd.DataFrame(temp)\n",
        "x = np.array(df['image'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIW4nDd58zsj"
      },
      "outputs": [],
      "source": [
        "# prepare the labels test and training set\n",
        "labels = tf.convert_to_tensor(df['Class'][:-200])\n",
        "tlabels = tf.convert_to_tensor(df['Class'][-200:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDVnji6__Vwv"
      },
      "outputs": [],
      "source": [
        "# prepare the features train and test set\n",
        "features = tf.stack(x[:-200])\n",
        "tfeatures = tf.stack(x[-200:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0JrpLQa7FSo"
      },
      "outputs": [],
      "source": [
        "# make the model (CNN). We add only 1 Convolutionary layer\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(256,256,3)),\n",
        "    tf.keras.layers.Rescaling(1/255.0), # this layes re schales the input to range of 0-1 for saving computation further\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax') # softmax because there are multiple classes\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spb2ju_c1udz",
        "outputId": "a8619c83-9a5c-4dbd-a1e7-5ed41718614f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling (Rescaling)       (None, 256, 256, 3)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 115200)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                7372864   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,466,437\n",
            "Trainable params: 7,466,437\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam',loss=losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INDKauQI47e3",
        "outputId": "7def0985-76ec-492d-a307-3fb4e88f5e52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fit model on training data\n",
            "Epoch 1/3\n",
            "55/55 [==============================] - 485s 9s/step - loss: 1.4279 - accuracy: 0.4207\n",
            "Epoch 2/3\n",
            "55/55 [==============================] - 471s 9s/step - loss: 1.0174 - accuracy: 0.6089\n",
            "Epoch 3/3\n",
            "55/55 [==============================] - 475s 9s/step - loss: 0.8171 - accuracy: 0.6919\n"
          ]
        }
      ],
      "source": [
        "# By Adding more layers the accuracy increased by about 8%\n",
        "# Also did pooling because the parameters were becoming way too large\n",
        "print(\"Fit model on training data\")\n",
        "history = model.fit(\n",
        "    features,\n",
        "    labels,\n",
        "    batch_size=64,\n",
        "    epochs=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRI1ELipJ0qq",
        "outputId": "f4c2ea7a-bea9-48a7-adf4-fd52910616a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 8s 2s/step - loss: 0.9030 - accuracy: 0.6482\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(tfeatures, tlabels, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3HXbeE1Qivw",
        "outputId": "28d7da38-5e94-4b1b-c840-27c337000184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After increasing the dataset size from 1000 -> 2000, the accuracy increased by 15%\n",
            "I didnt do data augmentation because i wasnt short of training data\n"
          ]
        }
      ],
      "source": [
        "print(\"After increasing the dataset size from 1000 -> 2000, the accuracy increased by 15%\")\n",
        "print(\"I didnt do data augmentation because i wasnt short of training data\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
